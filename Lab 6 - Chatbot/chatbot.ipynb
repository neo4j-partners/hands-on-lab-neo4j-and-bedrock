{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d8979e",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "In this notebook, we'll build a chatbot.  The chatbot interface will use gradio.  Underlying the chatbot is the Neo4j knowledge graph we built in previous labs.  The chatbot uses generative AI and langchain. Here is the flow:\n",
    "1. User asks a question in natural language.\n",
    "2. Generative AI converts it to Neo4j Cypher.\n",
    "3. Cypher query queries the Neo4j knowledge graph for deeper contextual facts.\n",
    "4. Generative AI converts the database response to natural language.\n",
    "5. Response is presented to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469d977",
   "metadata": {},
   "source": [
    "Let's begin by installing the packages needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages \n",
    "\n",
    "# Core LangChain and related dependencies\n",
    "%pip install -U 'langchain>=0.3.0,<0.4.0'  # Core LangChain (ensures latest compatible version)\n",
    "%pip install -U 'langchain-neo4j'      # Neo4j integration for LangChain\n",
    "%pip install -U 'langchain-aws'        # AWS Bedrock integration for LangChain\n",
    "%pip install -U 'langchain-community>=0.3.20' # Langchain community packages.\n",
    "%pip install -U 'langsmith>=0.1.125'     # Langsmith package.\n",
    "%pip install -U 'openai'                   # langchain has a dependency on openai\n",
    "%pip install -U 'tenacity>=9.0'          # Retry mechanism for robustness\n",
    "%pip install -U 'anyio>=4.4.0'           # Asynchronous I/O support\n",
    "\n",
    "# Data processing and UI\n",
    "%pip install pandas<2.0.0            # Specific Pandas version for compatibility\n",
    "%pip install gradio                   # Gradio for building web interfaces\n",
    "\n",
    "# Utility packages\n",
    "%pip install python-dotenv            # For managing environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a6136",
   "metadata": {},
   "source": [
    "Now restart the kernel. That will allow the Python evironment to import the new packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea81aba-c3e5-435b-9a9a-86d0a65da844",
   "metadata": {},
   "source": [
    "## Base Example Without Grounding\n",
    "Before grounding with the Neo4j, let's setup up a baseline that just uses an LLM to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1b0b6-57c6-427f-b007-1419d1213dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_NAME = 'bedrock-runtime'\n",
    "REGION_NAME = 'us-east-1'\n",
    "\n",
    "import boto3\n",
    "bedrock = boto3.client(\n",
    " service_name=SERVICE_NAME,\n",
    " region_name=REGION_NAME,\n",
    " endpoint_url=f'https://{SERVICE_NAME}.{REGION_NAME}.amazonaws.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d45b7-1406-4454-8811-67a2b351c7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "base_chain = BedrockLLM(\n",
    "                model_id=\"anthropic.claude-v2\",\n",
    "                client=bedrock,\n",
    "                model_kwargs = {\n",
    "                    \"temperature\":0,\n",
    "                    \"top_k\":1, \"top_p\":0.1,\n",
    "                    \"anthropic_version\":\"bedrock-2023-05-31\",\n",
    "                    \"max_tokens_to_sample\": 2048\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6d342-e9dc-4533-955e-601e181011ae",
   "metadata": {},
   "source": [
    "We can now ask a simple finance question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933a2d9-53fd-41df-8b5a-248885b0dbda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_response = base_chain.invoke(\"\"\"What are the top 10 investments for Blackrock?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f64a21-8347-4eec-a83a-8044f824a3d8",
   "metadata": {},
   "source": [
    "While this answer looks reasonable, we have no real way to know how the LLM came it with it, or where it was sourced from.\n",
    "\n",
    "Here is a more complicated example where we expect the LLM to understand some more specific terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43c396-39d5-4511-bc3d-a56a1880afc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_response = base_chain.invoke(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cf10e-45f4-427d-bda6-a7ae4c706d92",
   "metadata": {},
   "source": [
    "In this case, it looks like the LLM understands the ubiquitous acronym FAANG but, unsurprisingly, the results indicate it doesn't understand manager within the context of our data model.  In your use case, you may have lots of specific terminology/ontology like this that you would need a chatbot to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a087cb5-f99d-4c7c-bdac-14ec8cd411cb",
   "metadata": {},
   "source": [
    "## Grounding LLMs with Neo4j\n",
    "Now let's create a chatbot that is grounded with Neo4j. Below is the pattern we will follow with LangChain:\n",
    "\n",
    "![](images/langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044359a",
   "metadata": {},
   "source": [
    "## Cypher Generation\n",
    "We have to use a prompt template that: \n",
    "1. Clearly states what schema to use \n",
    "2. Provides principles the chatbot should follow in generating responses\n",
    "3. Demonstrates few-shot examples to help the chatbot be more accurate in its query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5c4a7-9821-4fe8-9b26-49dc96d89103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "<instructions>\n",
    "* Use aliases to refer the node or relationship in the generated Cypher query\n",
    "* Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "* Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "* Use only Nodes and relationships mentioned in the schema\n",
    "* Always enclose the Cypher output inside 3 backticks (```)\n",
    "* Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "* Use the relationship variable `o` to access the `shares` and `value` properties of the `OWNS` relationship when calculating the sums.\n",
    "* Cypher is NOT SQL. So, do not mix and match the syntaxes\n",
    "* Use the elementId() function instead of id() to compare node identifiers\n",
    "</instructions>\n",
    "\n",
    "Strictly use this Schema for Cypher generation:\n",
    "<schema>\n",
    "{schema}\n",
    "</schema>\n",
    "\n",
    "The samples below follow the instructions and the schema mentioned above. So, please follow the same when you generate the cypher:\n",
    "<samples>\n",
    "Human: Which fund manager owns most shares? What is the total portfolio value?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, sum(distinct o.shares) as ownedShares, sum(o.value) as portfolioValue ORDER BY ownedShares DESC LIMIT 10```\n",
    "\n",
    "Human: Which fund manager owns most companies? How many shares?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, count(distinct c) as ownedCompanies, sum(distinct o.shares) as ownedShares ORDER BY ownedCompanies DESC LIMIT 10```\n",
    "\n",
    "Human: What are the top 10 investments for Vanguard?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" RETURN c.companyName as Investment, sum(DISTINCT o.shares) as totalShares, sum(DISTINCT o.value) as investmentValue order by investmentValue desc limit 10```\n",
    "\n",
    "Human: What other fund managers are investing in same companies as Vanguard?\n",
    "Assistant: ```MATCH (m1:Manager) -[o1:OWNS]-> (c1:Company) <-[o2:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) contains \"vanguard\" AND elementId(m1) <> elementId(m2) RETURN m2.managerName as manager, sum(DISTINCT o2.shares) as investedShares, sum(DISTINCT o2.value) as investmentValue ORDER BY investmentValue LIMIT 10```\n",
    "\n",
    "Human: What are the top 10 investments for rempart?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"rempart\" RETURN c.companyName as Investment, sum(DISTINCT o.shares) as totalShares, sum(DISTINCT o.value) as investmentValue order by investmentValue desc limit 10```\n",
    "\n",
    "Human: What are the top investors for Apple?\n",
    "Assistant: ```MATCH (m1:Manager) -[o:OWNS]-> (c1:Company) WHERE toLower(c1.companyName) contains \"apple\" RETURN distinct m1.managerName as manager, sum(o.value) as totalInvested ORDER BY totalInvested DESC LIMIT 10```\n",
    "\n",
    "Human: What are the other top investments for fund managers investing in Apple?\n",
    "Assistant: ```MATCH (c1:Company) <-[o1:OWNS]- (m1:Manager) -[o2:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND elementId(c1) <> elementId(c2) RETURN DISTINCT c2.companyName as company, sum(o2.value) as totalInvested, sum(o2.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10```\n",
    "\n",
    "Human: What are the top investors in the last 3 months?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE date() > o.reportCalendarOrQuarter > o.reportCalendarOrQuarter - duration({{months:3}}) RETURN distinct m.managerName as manager, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10```\n",
    "\n",
    "Human: What are top investments in last 6 months for Vanguard?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:6}}) RETURN distinct c.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10```\n",
    "\n",
    "Human: Who are Apple's top investors in last 3 months?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(c.companyName) contains \"apple\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:3}}) RETURN distinct m.managerName as investor, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10```\n",
    "\n",
    "Human: Which fund manager under 200 million has similar investment strategy as Vanguard?\n",
    "Assistant: ```MATCH (m1:Manager) -[o1:OWNS]-> (:Company) <-[o2:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) CONTAINS \"vanguard\" AND elementId(m1) <> elementId(m2) WITH distinct m2 AS m2, sum(distinct o2.value) AS totalVal WHERE totalVal < 200000000 RETURN m2.managerName AS manager, totalVal*0.000001 AS totalVal ORDER BY totalVal DESC LIMIT 10```\n",
    "\n",
    "Human: Who are common investors in Apple and Amazon?\n",
    "Assistant: ```MATCH (c1:Company) <-[o1:OWNS]- (m:Manager) -[o2:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND toLower(c2.companyName) CONTAINS \"amazon\" RETURN DISTINCT m.managerName LIMIT 50```\n",
    "\n",
    "Human: What are Vanguard's top investments by shares for 2023?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.value) AS totalValue ORDER BY totalValue DESC LIMIT 10```\n",
    "\n",
    "Human: What are Vanguard's top investments by value for 2023?\n",
    "Assistant: ```MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.shares) AS totalShares ORDER BY totalShares DESC LIMIT 10```\n",
    "</samples>\n",
    "\n",
    "Human: {question}\n",
    "Assistant: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e46b3",
   "metadata": {},
   "source": [
    "Now let’s create a LangChain prompt template.  \n",
    "\n",
    "This template defines the parameter inputs for the prompt sent to the Cypher generation bot.  In our example, the inputs will be schema and question.  The question comes from the end user.  The LangChain GraphCypherQAChain automatically inserts the schema via a built-in method to Neo4jGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86addda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=['schema','question'], validate_template=True, template=CYPHER_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f63c46",
   "metadata": {},
   "source": [
    "Now we'll load up the Aura credentials from the credential file we created in Lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7bc10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "dotenv_file = \"../aura_connection.txt\"\n",
    "load_dotenv(dotenv_file)\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "print('NEO4J_URI:', NEO4J_URI)\n",
    "print('NEO4J_USERNAME:', NEO4J_USERNAME)\n",
    "print('NEO4J_PASSWORD:', NEO4J_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e2e7d",
   "metadata": {},
   "source": [
    "We need to connect to the graph via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc48f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI, \n",
    "    username=NEO4J_USERNAME, \n",
    "    password=NEO4J_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1263e2",
   "metadata": {},
   "source": [
    "We define our `chain` object (specifically a`GraphCypherQAChain`) using Anthropic Claude V2 LLM.\n",
    "\n",
    "`GraphCypherQAChain` also takes a ‘Neo4jGraph’ so it can handle the chatbot process end-to-end, from taking the user question and translating to Cypher to executing the query, getting results, translating back to natural language, and returning to the user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94866b-7729-46a5-85cb-c11fb364eefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph, GraphCypherQAChain\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_aws import BedrockLLM\n",
    "import json\n",
    "\n",
    "llm = BedrockLLM(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    client=bedrock,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_k\": 1,\n",
    "        \"top_p\": 0.1,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens_to_sample\": 2048,\n",
    "    },\n",
    ")\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph,  # Use Neo4jGraph directly\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    verbose=True,\n",
    "    return_direct=True,\n",
    "    allow_dangerous_requests=True,\n",
    ")\n",
    "\n",
    "def chat(que):\n",
    "    r = chain.invoke(que)\n",
    "    print(r)\n",
    "    summary_prompt_tpl = f\"\"\"Human: \n",
    "    Fact: {json.dumps(r['result'])}\n",
    "\n",
    "    * Summarise the above fact as if you are answering this question \"{r['query']}\"\n",
    "    * When the fact is not empty, assume the question is valid and the answer is true\n",
    "    * Do not return helpful or extra text or apologies\n",
    "    * Just return summary to the user. DO NOT start with Here is a summary\n",
    "    * List the results in rich text format if there are more than one results\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    return llm.invoke(summary_prompt_tpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1590f",
   "metadata": {},
   "source": [
    "Below we have a few examples of how we can get answers from the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed32736-43f8-4c9c-b90e-69932461bbd3",
   "metadata": {},
   "source": [
    "## Why Ground Your LLM?\n",
    "Recall our base example where we asked for the top 10 Rempart investments?  We got an answer that looked like it may be reasonable, but we couldn't validate it or track sources.  We also asked what managers own FAANG stocks, and for that, we unsurprisingly received the wrong answers for our use case.\n",
    "\n",
    "Let's try again grounding with Neo4j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e6bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2 = chat(\"\"\"What are the top 10 investments for Vanguard?\"\"\")\n",
    "print(f\"Final answer: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65434f3-f818-46ca-9fc8-0daa4a77942b",
   "metadata": {},
   "source": [
    "Notice that this answer is different from our base example, and this time we have the Cypher logic used to obtain the answer from our database. This means that we can trace back how we came up with this answer and make any adjustments to our database or prompt if we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7da574-8c23-4c78-b35f-bb4e86cdb291",
   "metadata": {},
   "source": [
    "Now lets try the FAANG question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d1464-7202-4d66-af52-3a6ffe8668d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r3 = chat(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {r3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0686244-bb6b-4231-aebf-70b3c777ef25",
   "metadata": {},
   "source": [
    "Here again, we notice the traceability with Cypher, and because we engineered our prompt to include our schema, it understood what “manager” means in the context of our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d119d1-5b84-4665-985c-42eeeca49779",
   "metadata": {},
   "source": [
    "## Why Ground your LLM with Neo4j?\n",
    "There are 3 primary reasons to ground your LLM with Neo4j specifically:\n",
    "1. __Grounding for more complex question handling__: Multi-hop knowledge retrieval across connected data. Connections between data points are calculated before query time. \n",
    "2. __Enterprise reliability and security__: Fine-grained security so the chatbot only accesses information the user has permission to. Autonomous clustering for horizontal scaling.  Fully managed service in the cloud through Aura. \n",
    "3. __Performance__: fast queries with high concurrency for many users.\n",
    "\n",
    "We can explore point 1 with more complex questions below.\n",
    "\n",
    "A question requiring ~4 hops (would be joins in the relational world).  Having a knowledge graph with relationships calculated before query time allows us to answer the question quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r4 = chat(\"\"\"What are other top investments for fund managers investing in Lowes?\"\"\")\n",
    "print(f\"Final answer: {r4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbca6ab-01d6-4697-a58f-7978567ff66b",
   "metadata": {},
   "source": [
    "and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588955da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r5 = chat(\"\"\"Please get me common investors between Tesla and Costco\"\"\")\n",
    "print(f\"Final answer: {r5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b038e",
   "metadata": {},
   "source": [
    "## Grounded Chatbot\n",
    "Now we will use Gradio to deploy a chat interface with our chain behind it.\n",
    "\n",
    "The below code deploys a Gradio application.  You can access the app via a local URL. A publicly sharable URL is also provided (sharable for 3 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe1766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferWindowMemory, ChatMessageHistory\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    chat_memory=chat_history,  # Correct integration\n",
    "    k=5,  # Number of interactions to keep in memory.\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key='output'\n",
    ")\n",
    "\n",
    "def chat_response(input_text, history):\n",
    "    try:\n",
    "        return chat(input_text)\n",
    "    except:\n",
    "        return \"I'm sorry, there was an error retrieving the information you requested.\"\n",
    "\n",
    "interface = gr.ChatInterface(\n",
    "    fn=chat_response,\n",
    "    title=\"Investment Chatbot\",\n",
    "    description=\"powered by Neo4j\",\n",
    "    theme=\"soft\",\n",
    "    chatbot=gr.Chatbot(height=500, type=\"messages\"),\n",
    "    examples=[\n",
    "        \"What are the top 10 investments for Vanguard?\",\n",
    "        \"Which manager owns FAANG stocks?\",\n",
    "        \"What are other top investments for fund managers investing in Exxon?\",\n",
    "        \"What are Rempart's top investments by value for 2023?\",\n",
    "        \"Who are the common investors between Tesla and Costco?\",\n",
    "    ],\n",
    "    additional_inputs=None,\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5197afd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we went through the steps of connecting a LangChain agent to a Neo4j database and using it to generate Cypher queries in response to user requests via LLMs on Bedrock, thus grounding the LLM with a knowledge graph.\n",
    "\n",
    "While we used the Anthropic `claude v2` model here, this approach can be generalized to other Bedrock LLMs.  This process can also be augmented with additional steps around the generation chain to customize the experience for specific use cases.  \n",
    "\n",
    "The critical takeaway is the importance of Neo4j Knowledge Graph as a grounding database to: \n",
    "* Anchor your chatbot to reality as it generates responses and \n",
    "* Enable your LLM to provide answers enriched with relevant enterprise data."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
